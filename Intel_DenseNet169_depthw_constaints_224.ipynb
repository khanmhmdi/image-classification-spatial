{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Intel-DenseNet169-depthw-constaints-224.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mr7495/image-classification-spatial/blob/main/Intel_DenseNet169_depthw_constaints_224.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t1SzEKNRzFL5"
      },
      "source": [
        "!nvidia-smi #show GPU type"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6mw_i4iuzHlC"
      },
      "source": [
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import cv2\n",
        "import zipfile\n",
        "import shutil\n",
        "import random\n",
        "import pandas as pd\n",
        "import csv\n",
        "import os"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uP8qFi9uWzAG"
      },
      "source": [
        "#download Intel image classification dataset from Kaggle (https://www.kaggle.com/puneet6060/intel-image-classification)\n",
        "#Get a new download like from kaggle website at the mentioned URL, then replace the link with the \"kaggle_linke\" input in the next line of code \n",
        "\n",
        "!wget -cO - kaggle_link > intel.zip #replace kaggle_link with a new download link from https://www.kaggle.com/puneet6060/intel-image-classification"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pWNFXlWBWzDg"
      },
      "source": [
        "archive = zipfile.ZipFile('intel.zip') #Unzip Kaggle Dataset\n",
        "for file in archive.namelist():\n",
        "     archive.extract(file, 'data')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XnCBwl-5zM4U"
      },
      "source": [
        "#Extract Training and Testing data\n",
        "classes=['buildings','forest','glacier','mountain','sea','street']\n",
        "\n",
        "train_data=[]\n",
        "for r,d,f in os.walk('data/seg_train/seg_train'):\n",
        "  for file in f:\n",
        "    if '.jpg' in file:\n",
        "      for clas in classes:\n",
        "        if clas in r:\n",
        "          train_data.append(['{}/{}'.format(r[5:],file),clas])\n",
        "          break\n",
        "\n",
        "test_data=[]\n",
        "for r,d,f in os.walk('data/seg_test/seg_test'):\n",
        "  for file in f:\n",
        "    if '.jpg' in file:\n",
        "      for clas in classes:\n",
        "        if clas in r:\n",
        "          test_data.append(['{}/{}'.format(r[5:],file),clas])\n",
        "          break"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CnE4c-VwzNEt"
      },
      "source": [
        "#write train and test CSV files\n",
        "with open('data/train.csv','w',newline='') as f:\n",
        "  csvw=csv.writer(f)\n",
        "  csvw.writerow(['filename','class'])\n",
        "  for item in train_data:\n",
        "    class_name=item[1]\n",
        "    img_name=item[0]\n",
        "    csvw.writerow([img_name,class_name])\n",
        "\n",
        "with open('data/test.csv','w',newline='') as f:\n",
        "  csvw=csv.writer(f)\n",
        "  csvw.writerow(['filename','class'])\n",
        "  for item in test_data:\n",
        "    class_name=item[1]\n",
        "    img_name=item[0]\n",
        "    csvw.writerow([img_name,class_name])"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79KB2ItAzHqb"
      },
      "source": [
        "#Set data augmentation techniques\n",
        "\n",
        "train_datagen = keras.preprocessing.image.ImageDataGenerator(horizontal_flip=True,vertical_flip=True\n",
        "                                                             ,zoom_range=0.2,rotation_range=360\n",
        "                                                             ,width_shift_range=0.1,height_shift_range=0.1\n",
        "                                                             ,channel_shift_range=50\n",
        "                                                             ,brightness_range=(0,1.2)\n",
        "                                                             ,preprocessing_function=keras.applications.imagenet_utils.preprocess_input)\n",
        "\n",
        "test_datagen = keras.preprocessing.image.ImageDataGenerator(preprocessing_function=keras.applications.imagenet_utils.preprocess_input)\n",
        "\n",
        "train_df = pd.read_csv(\"data/train.csv\")\n",
        "test_df = pd.read_csv(\"data/test.csv\")"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L3dqLamH0TBw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6aede7e8-fc13-4f0b-903e-a8ce222782eb"
      },
      "source": [
        "#Create Data augmentation techniques\n",
        "\n",
        "batch_size=70\n",
        "train_generator = train_datagen.flow_from_dataframe(\n",
        "      dataframe=train_df,\n",
        "      directory='data',\n",
        "      x_col=\"filename\",\n",
        "      y_col=\"class\",\n",
        "      target_size=(224, 224),\n",
        "      batch_size=batch_size,\n",
        "      class_mode='categorical',shuffle=True)\n",
        "validation_generator = test_datagen.flow_from_dataframe(\n",
        "        dataframe=test_df,\n",
        "        directory='data',\n",
        "        x_col=\"filename\",\n",
        "        y_col=\"class\",\n",
        "        target_size=(224, 224),\n",
        "        batch_size=batch_size,\n",
        "        class_mode='categorical',shuffle=True)\n",
        "train_img_num=len(train_generator.filenames)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 14034 validated image filenames belonging to 6 classes.\n",
            "Found 3000 validated image filenames belonging to 6 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yo9nEsSx7OfK"
      },
      "source": [
        "name=\"Intel-DenseNet169-depthw-constaints-224\"\n",
        "!mkdir \"models\" #create new folder for saving checkpoints\n",
        "!mkdir \"reports\" #create new folder for saving evaluation reports\n",
        "keras.backend.clear_session() #clear backend\n",
        "shape=(224,224,3) \n",
        "input_tensor=keras.Input(shape=shape)\n",
        "base_model=keras.applications.DenseNet169(input_tensor=input_tensor,weights=None,include_top=False)\n",
        "depth=keras.layers.DepthwiseConv2D(tuple(base_model.output.shape[1:3]), depthwise_initializer=keras.initializers.RandomNormal(mean=0.0,stddev=0.01),\n",
        "                                      bias_initializer=keras.initializers.Zeros(),depthwise_constraint=keras.constraints.NonNeg())(base_model.output)\n",
        "flat=keras.layers.Flatten()(depth)\n",
        "preds=keras.layers.Dense(6,activation='softmax',\n",
        "                          kernel_initializer=keras.initializers.RandomNormal(mean=0.0,stddev=0.01),\n",
        "                          bias_initializer=keras.initializers.Zeros(),)(flat)\n",
        "model=keras.Model(inputs=base_model.input, outputs=preds)  \n",
        "\n",
        "##################################\n",
        "for layer in model.layers:\n",
        "  layer.trainable = True\n",
        "model.summary()\n",
        "filepath=\"models/%s-{epoch:02d}-{val_accuracy:.4f}.hdf5\"%name\n",
        "checkpoint = keras.callbacks.ModelCheckpoint(filepath, monitor='val_accuracy', save_best_only=False, mode='max') #creating checkpoint to save the best validation accuracy\n",
        "callbacks_list = [checkpoint]\n",
        "\n",
        "#Determine adaptive learning rate with an initialization value of 0.045 and decay of 0.94 every two epochs.\n",
        "lr_schedule =keras.optimizers.schedules.ExponentialDecay(\n",
        "    initial_learning_rate=0.045,\n",
        "    decay_steps=2*int(train_img_num/batch_size),\n",
        "    decay_rate=0.94,\n",
        "    staircase=True)\n",
        "optimizer=keras.optimizers.SGD(momentum=0.9,learning_rate=lr_schedule)\n",
        "model.compile(optimizer=optimizer, loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "hist=model.fit_generator(train_generator, epochs=148,validation_data=validation_generator,shuffle=True,callbacks=callbacks_list) #start training\n",
        "with open('reports/{}.csv'.format(name), mode='w',newline='') as csv_file: #write reports\n",
        "  csv_writer = csv.writer(csv_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
        "  for key in hist.history:\n",
        "    data=[key]\n",
        "    data.extend(hist.history[key])\n",
        "    csv_writer.writerow(data)\n",
        "print(\"Training finished. Reports saved!\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}